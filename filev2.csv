x,y
1, Optimizer:responsible for updating the model's weights during training to minimize the defined loss function. It determines how the model's parameters are adjusted in response to the training data and the calculated gradients.
2, Different optimizers have different algorithms for weight updates and they can impact the training process's speed and effectiveness. Common optimizers include stochastic gradient descent (SGD) Adam RMSprop and more.
3, Loss Function:(known as the objective or cost function) measures the discrepancy between the predicted values of the model and the actual target values in the training data.
4, The choice of loss function depends on the nature of the problem you are solving. For example mean squared error (MSE) is commonly used for regression tasks while categorical cross-entropy is used for classification tasks.
5, Metrics: are used to evaluate the model's performance during training and can help you understand how well the model is doing on tasks beyond just minimizing the loss.
6, Gradiant Descent: fundamental optimization algorithm used in ML and DL to minimize a loss function and update the parameters (weights) of a model. 
7, Batch Gradient Descent: the entire training dataset is used to compute the gradient of the loss function with respect to the model parameters in each iteration.
8, Stochastic Gradient Descent (SGD): only one randomly selected training example is used to compute the gradient and update the model parameters in each iteration.
9, Mini-Batch Gradient Descent: strikes a balance between batch gradient descent and stochastic gradient descent. It divides the training dataset into smaller fixed-sized mini-batches.
10, The choice of which gradient descent variant to use depends on factors like the dataset size available computational resources and the specific problem being solved.
11, Epoch: is a complete pass through the entire training dataset during the training of a machine learning or deep learning model. In other words it's one full iteration over all training examples.
12, Batch Size: refers to the number of training examples used in each iteration (or mini-batch) during the training process.
13, The relationship between the number of epochs and the batch size determines how many parameter updates are made during training.
14, Learning Rate: It plays a fundamental role in determining the step size at which the model's parameters (weights and biases) are updated during training. 
15, Role of Learning: Control the Step Size Balance Between Speed and Stability Hyperparameter Tuning Adaptive Learning Rates Learning Rate Scheduling.
16, Network Used for Training: During the training phase the network is exposed to the training dataset which consists of input data and corresponding target labels.
17, Network Used for Testing: Once the training is completed the trained network is used for making predictions on new unseen data which is called the testing or inference phase.
18, Straightforward method for updating the weights in a neural network is the basic form of the stochastic gradient descent (SGD) optimization algorithm.
19, Initialize Weights-Iterative Weight Update-Forward Pass-Compute Loss-Backpropagation-Update Weights-Repeat.
20, Forward pass- input data is processed through the network's layers- and predictions or activations are computed layer by layer until the final output is generated. 
21, Backpropagation: It is the foundation for learning the optimal weights and biases that minimize the loss function and enable the network to make accurate predictions.
22, Fully Connected: it means that every neuron in one layer is connected to every neuron in the adjacent layer. This connectivity pattern is also known as a "dense" or "fully connected layer."
23, Deep learning is a subfield of machine learning that focuses on the use of artificial neural networks- specifically deep neural networks- to solve complex tasks. Deep neural networks are characterized by their deep architecture- which means they have multiple layers
24, In simple machine learning- such as traditional linear regression or decision tree models- the models typically have a shallow architecture with fewer layers. 
25, new_weight = old_weight - learning_rate * derivative
26, Gradiant: the gradient is a vector that contains the partial derivatives of the loss function with respect to the model's parameters. It guides the optimization process- allowing the model to iteratively update its parameters in a way that minimizes the loss and improves its predictive performance.
27, The vanishing gradient problem occurs when the gradients of the loss function with respect to the model's parameters become extremely small (close to zero) as they are backpropagated through the network during training.
28, The exploding gradient problem is the opposite of the vanishing gradient problem. It occurs when gradients become extremely large (growing exponentially) as they are backpropagated through the network.
29, dL/dw1 = (y_Pred - y_true) * (sigmoid(Z) * (1 - sigmoid(Z))) * x1
30, (train_images- train_labels)/ (test_images- test_labels) = mnist.load_data() / train_images = train_images.reshape((60000- 28 * 28)) / train_images = train_images.astype('float32') / 255 / test_images = test_images.reshape((10000- 28 * 28)) / test_images = test_images.astype('float32') / 255 / from keras.utils import to_categorical / train_labels = to_categorical(train_labels) / test_labels = to_categorical(test_labels) / network = Sequential() / network.add(Dense(512- activation='relu'- input_shape=(28 * 28-))) / network.add(Dense(10- activation='softmax')) / network.compile(optimizer='rmsprop'-loss='categorical_crossentropy'-metrics=['accuracy']) / network.fit(train_images- train_labels- epochs=7- batch_size=64)
31, np.random.seed(9261)   X1  y1 = np.random.randint(0  24  (3  4))  np.random.random(3)  W1 = [1 0 2 1] #weights  b1 = [-4] #bias z1 = np.dot(W1  X1.T) + b1 #fordware propagation funtion a1 = np.tanh(z1) #activation funtion  mse1 = np.mean((a1-y1)**2) #mean squared error
32, np.random.seed(789)   X2 = np.array([[2 -1 3] [-4 1 2]])   W2 = np.random.randint(-5  25 (1 3))   b2 = [1]     z2 = np.dot(W2  X2.T) + b     a2 = 1/(1+np.exp(-X2))
33, You get your data by first running np.random.seed(1230) and then creating the array X = np.random.randint(0- 255- (2932- 109- 99)). when the data is scaled and converted to the correct dimension for a fully connected network- what is the new 1160(st/nd/rd/th) pixel value for the 464(st/nd/rd/th) image?
34, np.random.seed(1230)  X = np.random.randint(0- 255- (2932- 109- 99))    print(X[463- 1160 // 99- 1160 % 99] / 255)  R=0.23137254901960785
35, You have image data that comes in an array of dimension (31511 47 58). This data needs to be fed into a fully-connected neural network.  What array dimensions does your data need?   R=(31511 2726)
36, A fully connected neural network is constructed with architecture the first hidden layer with 3 neurons.  the second hidden layer with 4 neurons and the third layer (output layer) contains 1 neuron.  it takes a input from a sample X where each sample contains 5 features. 
37, What would be the shape of the first hidden layers weight matrix given the representation of output from layer 1 is given by a[1] = sigma (W[1]]. x + B[1])) R= (3- 5)
38, when you add a dropout layer waht would be the output of the deactivated neuron during the forward pass? R=0
39, Short of what is defined as ReLu(x)=max(0 x)   R=Rectified linear unit
40, What two pieces of information does the derivative of the loss function provide? R: The direction and magnitude with which the network parameters can be updated
41, Neuron = Neuron_Relu(W- b).feedforward(X)
42, NN = OurNeuralNetwork(W1- W2- b1- b2) / NN.feedforward(X)
43, Explan the use of the loss function. R: Regression Loss Function: Mean Squeared Error / Classification Loss Function: Cross-Entropy Loss
44, Mini-batch gradient descent is when model parameters are updated after. R: A specified subset of training samples is processed
45, How many trainable parameters does the following network have. R: features = 511 / neuronLayer1 = 29 / bias1 = 29 / neuronLayer2 = 7 / bias2 =7 / neuronLayer3 = 8 / bias3 = 8 / neuronLayerOutput = 6 / biasOutput = 6 / print(features*neuronLayer1 + bias1 + neuronLayer2*neuronLayer1 + bias2 + neuronLayer3*neuronLayer2 + bias3 + neuronLayerOutput*neuronLayer3 + biasOutput)
46, X = np.random.randint(0- 1- (5000- 11-13)) / y = np.random.randint(0- 5- (5000)) / from tensorflow.keras.models import Sequential / from tensorflow.keras.layers import Dense / X = X.reshape(5000- 11*13) / model = Sequential() / model.add(Dense(300- input_shape=(11*13-)- activation='relu')) / model.add(Dense(120- activation='relu')) / model.add(Dense(124- activation='relu')) / model.add(Dense(5- activation='softmax')) / model.summary()
47, w_old = w / n_iterations = 5 / for i in range(n_iterations): / total = w * x + b / y_hat = 1/(1+np.exp(-total)) / L = (y - y_hat)**2  / dL_dw = -2 * (y - y_hat)*y_hat*(1-y_hat)*x / w = w - lr*dL_dw  / diff = w - w_old / print(f"new weight: {w:.4f}  w_old: {w_old:.4f}    dL_dw: {dL_dw:.4f}    lr * dL_dw: {lr*dL_dw:.4f}    diff: {diff:.4f}   pred = y_hat = wx =: {y_hat:.4f}  Loss: {L:.4f}") / w_old = w