X, Y
1, Optimizer: The optimizer is responsible for updating the model's weights during training to minimize the defined loss function. It determines how the model's parameters are adjusted in response to the training data and the calculated gradients.
2, Different optimizers have different algorithms for weight updates and they can impact the training process's speed and effectiveness. Common optimizers include stochastic gradient descent (SGD) Adam RMSprop and more.
3, Loss Function: The loss function (also known as the objective or cost function) measures the discrepancy between the predicted values of the model and the actual target values in the training data.
4, The choice of loss function depends on the nature of the problem you are solving. For example mean squared error (MSE) is commonly used for regression tasks while categorical cross-entropy is used for classification tasks.
5, Metrics: Metrics are used to evaluate the model's performance during training and can help you understand how well the model is doing on tasks beyond just minimizing the loss.
6, Gradiant Descent: fundamental optimization algorithm used in machine learning and deep learning to minimize a loss function and update the parameters (weights) of a model. 
7, Batch Gradient Descent:the entire training dataset is used to compute the gradient of the loss function with respect to the model parameters in each iteration.
8, Stochastic Gradient Descent (SGD): only one randomly selected training example is used to compute the gradient and update the model parameters in each iteration.
9, Mini-Batch Gradient Descent: strikes a balance between batch gradient descent and stochastic gradient descent. It divides the training dataset into smaller fixed-sized mini-batches.
10, The choice of which gradient descent variant to use depends on factors like the dataset size available computational resources and the specific problem being solved.
11, Epoch: An epoch is a complete pass through the entire training dataset during the training of a machine learning or deep learning model. In other words it's one full iteration over all training examples.
12, Batch Size: The batch size refers to the number of training examples used in each iteration (or mini-batch) during the training process.
13, The relationship between the number of epochs and the batch size determines how many parameter updates are made during training.
14, Learning Rate: It plays a fundamental role in determining the step size at which the model's parameters (weights and biases) are updated during training. 
15, Role of Learning: Control the Step Size Balance Between Speed and Stability Hyperparameter Tuning Adaptive Learning Rates Learning Rate Scheduling.
16, Network Used for Training: During the training phase the network is exposed to the training dataset which consists of input data and corresponding target labels.
17, Network Used for Testing: Once the training is completed the trained network is used for making predictions on new unseen data which is called the testing or inference phase.
18, Straightforward method for updating the weights in a neural network is the basic form of the stochastic gradient descent (SGD) optimization algorithm.
19, Initialize Weights-Iterative Weight Update-Forward Pass-Compute Loss-Backpropagation-Update Weights-Repeat.
20, Forward pass- input data is processed through the network's layers- and predictions or activations are computed layer by layer until the final output is generated. 
21, Backpropagation: It is the foundation for learning the optimal weights and biases that minimize the loss function and enable the network to make accurate predictions.
22, Fully Connected: it means that every neuron in one layer is connected to every neuron in the adjacent layer. This connectivity pattern is also known as a "dense" or "fully connected layer."
23, Deep learning is a subfield of machine learning that focuses on the use of artificial neural networks- specifically deep neural networks- to solve complex tasks. Deep neural networks are characterized by their deep architecture- which means they have multiple layers
24, In simple machine learning- such as traditional linear regression or decision tree models- the models typically have a shallow architecture with fewer layers. 
25, new_weight = old_weight - learning_rate * derivative
26, Gradiant: the gradient is a vector that contains the partial derivatives of the loss function with respect to the model's parameters. It guides the optimization process- allowing the model to iteratively update its parameters in a way that minimizes the loss and improves its predictive performance.
27, The vanishing gradient problem occurs when the gradients of the loss function with respect to the model's parameters become extremely small (close to zero) as they are backpropagated through the network during training.
28, The exploding gradient problem is the opposite of the vanishing gradient problem. It occurs when gradients become extremely large (growing exponentially) as they are backpropagated through the network.
29, dL/dw1 = (y_Pred - y_true) * (sigmoid(Z) * (1 - sigmoid(Z))) * x1
30, (train_images- train_labels)/ (test_images- test_labels) = mnist.load_data() / train_images = train_images.reshape((60000- 28 * 28)) / train_images = train_images.astype('float32') / 255 / test_images = test_images.reshape((10000- 28 * 28)) / test_images = test_images.astype('float32') / 255 / from keras.utils import to_categorical / train_labels = to_categorical(train_labels) / test_labels = to_categorical(test_labels) / network = Sequential() / network.add(Dense(512- activation='relu'- input_shape=(28 * 28-))) / network.add(Dense(10- activation='softmax')) / network.compile(optimizer='rmsprop'-loss='categorical_crossentropy'-metrics=['accuracy']) / network.fit(train_images- train_labels- epochs=7- batch_size=64)